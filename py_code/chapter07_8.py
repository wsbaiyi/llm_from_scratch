import psutil
import json
from tqdm import tqdm
import urllib.request

def format_input(entry):
    instruction_text = (
            f"Below is an instruction that describes a task."
            f"Write a response that appropriately completes the request."
            f"\n\n### Instruction:\n{entry['instruction']}"
    )
    input_text = f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""
    return instruction_text + input_text

def check_if_running(process_name):
    running = False
    for proc in psutil.process_iter(["name"]):
        if process_name in proc.info["name"]:
            running = True
            break
    return running

def query_model(prompt, model="llama3", url="http://localhost:11434/api/chat"):
    data = {
            "model": model,
            "seed": 123,       # for deterministic response
            "temperature": 0,  # for deterministic response
            "messages": [
                {"role": "user", "content": prompt}
            ]
    }
    payload = json.dumps(data).encode("utf-8")
    request = urllib.request.Request(url, data=payload, method="POST")
    request.add_header("Content-Type", "application/json")

    response_data = ""
    with urllib.request.urlopen(request) as response:
        while True:
            line = response.readline().decode("utf-8")
            if not line:
                break
            response_json = json.loads(line)
            response_data += response_json["message"]["content"]
    return response_data

def generate_model_scores(json_data, json_key, model="llama3"):
    scores = []
    for entry in tqdm(json_data, desc="Scoring entries"):
        prompt = (
                f"Given the input `{format_input(entry)}` "
                f"and correct output `{entry['output']}`, "
                f"score the model response `{entry[json_key]}`"
                f" on a scale from 0 to 100, where 100 is the best score. "
                f"Respond with the integer number only."
        )
        score = query_model(prompt, model)
        try:
            scores.append(int(score))
        except ValueError:
            print(f"Could not convert score: {score}")
            continue
    return scores

ollama_running = check_if_running("ollama")

if not ollama_running:
    raise RuntimeError("Ollama not running. Launch ollama before proceeding.")
print("Ollama running:", check_if_running("ollama"))

file_path = "instruction-data-with-json-response.json"
with open(file_path, "r") as file:
    test_data = json.load(file)

# model = "llama3"
# result = query_model("What do Llamas eat ah?", model)
# print(f"What do Llamas eat?\n >> {result}")
# 
# for entry in test_data[:3]:
#     prompt = (
#             f"Given the input `{format_input(entry)}` "
#             f"and correct output `{entry['output']}`, "
#             f"score the model response `{entry['model_response']}`"
#             f" on a scale from 0 to 100, where 100 is the best score. "
#     )
#     print("\nDataset response:")
#     print(">>", entry['output'])
#     print("\nModel response:")
#     print(">>", entry["model_response"])
#     print("\nScore:")
#     print(">>", query_model(prompt))
#     print("\n-----------------------")

scores = generate_model_scores(test_data, "model_response")
print(f"Number of scores: {len(scores)} of {len(test_data)}")
print(f"Average score: {sum(scores)/len(scores):.2f}\n")
