{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.1 A placeholder GPT model architecture class\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.embed=nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "        self.pos=nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
    "        \n",
    "        self.dropout=nn.Dropout(cfg['drop_rate'])\n",
    "        self.tf_block=nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "        )\n",
    "        self.layer_norm=DummyLayerNorm(cfg['emb_dim'])\n",
    "        self.fc=nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        b,token_len=x.shape\n",
    "        emb=self.embed(x)\n",
    "        pos=self.pos(torch.arange(token_len,device=x.device))\n",
    "        inputs=emb+pos\n",
    "        \n",
    "        x=self.dropout(inputs)\n",
    "        x=self.tf_block(x)\n",
    "        x=self.layer_norm(x)\n",
    "        x=self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "class DummyTransformerBlock(nn.Module):                                       #C\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):                                                     #D\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):                                              #E\n",
    "    def __init__(self, normalized_shape, eps=1e-5):                           #F\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6109, 3626, 6100,  345],\n",
       "        [6109, 3626, 6100,  345]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "tokenizer=tiktoken.get_encoding('gpt2')\n",
    "id1=torch.tensor(tokenizer.encode(txt1))\n",
    "id2=torch.tensor(tokenizer.encode(txt2))\n",
    "# 使用cat只会得到一维向量\n",
    "batch=torch.stack([id1,id1],dim=0)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [ 0.2673,  0.7159, -0.4726,  ..., -0.5923,  1.0530,  1.1042],\n",
      "         [ 0.2423,  1.3367, -0.5303,  ...,  1.8892, -0.5572,  0.1469],\n",
      "         [ 0.5900,  1.6046, -0.4373,  ...,  1.9740, -0.3847, -1.5081]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model=DummyGPTModel(GPT_CONFIG_124M)\n",
    "outputs=model(batch)\n",
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "data=torch.randn(2,5)\n",
    "change=nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
    "out=change(data)\n",
    "out.shape\n",
    "mean=out.mean(dim=-1,keepdim=True)\n",
    "var=out.var(dim=-1,keepdim=True)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output=(out-mean)/var.sqrt()\n",
    "# 关闭科学计数法\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(output.mean(dim=-1,keepdim=True))\n",
    "print(output.var(dim=-1,keepdim=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.2 A layer normalization class\n",
    "#与在数量batch维度上进行归一化的批量归一化不同，层归一化是在特征维度上进行归一化\n",
    "# 由于层归一化对每个输入的处理不依赖批量大小，因此在这些场景下提供了更高的灵活性和稳定性\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        #可以理解为我们在方差公式中直接用样本数 n 作为分母，不使用贝塞尔校正（通常分母使用 n−1 以校正样本方差估计中的偏差）。\n",
    "        #这种决定会导致所谓的有偏方差估计。对于大语言模型（LLM）来说，其嵌入维度 n 通常非常大，因此使用 n 和 n−1 的差异实际上可以忽略不计。\n",
    "        #我们选择这种方式是为了确保与 GPT-2 模型的归一化层兼容\n",
    "        var=x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        output=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        \n",
    "        return self.scale*output+self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
       "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln=LayerNorm(5)\n",
    "output=ln(data)\n",
    "print(output.mean(dim=-1,keepdim=True))\n",
    "print(output.var(dim=-1,keepdim=True,unbiased=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.3 An implementation of the GELU activation function\n",
    "# GELU激活函数\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=torch.linspace(-3,3,100)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,5))\n",
    "glu,rlu=GELU(),nn.ReLU()\n",
    "x1=glu(x)\n",
    "x2=rlu(x)\n",
    "for i,(title,data) in enumerate(zip(['glu','rlu'],[x1,x2])):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    plt.plot(x,data)\n",
    "    plt.title(title)\n",
    "plt.show()\n",
    "# ReLU 是一个分段线性函数，输入为正时输出输入值本身，否则输出零。而 GELU 是一种平滑的非线性函数，它近似于 ReLU，但在负值上也具有非零梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.4 A feed forward neural network module\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(\n",
    "            # 扩展后的高维空间可以让模型“看到”输入数据中更多的隐藏特征，提取出更丰富的信息。\n",
    "            # 然后在收缩回低维度时，这些丰富的特征被整合到了输入的原始维度表示中，使模型最终的输出包含更多的上下文和信息\n",
    "            nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg['emb_dim'],cfg['emb_dim'])\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)          #A\n",
    "out = ffn(x)\n",
    "print(out.shape)\n",
    "\n",
    "#A 创建一个 batch 大小为 2 的示例输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 快捷连接（也称跳跃连接或残差连接）的概念。快捷连接最初是在计算机视觉中的深度网络（尤其是残差网络）提出的，用于缓解梯度消失问题。\n",
    "# 梯度消失是指在训练中指导权重更新的梯度在反向传播过程中逐渐减小，导致早期层（靠近输入端的网络层）难以有效训练\n",
    "# Listing 4.5 A neural network to illustrate shortcut connections\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut=use_shortcut\n",
    "        self.layers=nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(nn.Linear(layer_sizes[0],layer_sizes[1]),GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[1],layer_sizes[2]),GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[2],layer_sizes[3]),GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[3],layer_sizes[4]),GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[4],layer_sizes[5]),GELU()),\n",
    "            ]\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            output=layer(x)\n",
    "            if self.use_shortcut and x.shape==output.shape:\n",
    "                x=output+x\n",
    "            else:\n",
    "                x=output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试不使用残差跳连的梯度\n",
    "model_without_shortcut=ExampleDeepNeuralNetwork(layer_sizes,False)\n",
    "# 使用残差跳连\n",
    "model_with_shortcut=ExampleDeepNeuralNetwork(layer_sizes,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义梯度\n",
    "def  get_gradiant(model,x):\n",
    "    output=model(x)\n",
    "    target=torch.tensor([[0.]])\n",
    "    \n",
    "    loss=nn.MSELoss()\n",
    "    l=loss(output,target)\n",
    "    \n",
    "    l.backward()\n",
    "    \n",
    "    \n",
    "    for name,weight in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f'name:{name} grad is {weight.grad.abs().mean().item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:layers.0.0.weight grad is 0.0006052075768820941\n",
      "name:layers.1.0.weight grad is 0.00036033481592312455\n",
      "name:layers.2.0.weight grad is 0.0021456123795360327\n",
      "name:layers.3.0.weight grad is 0.004196621477603912\n",
      "name:layers.4.0.weight grad is 0.015148940496146679\n"
     ]
    }
   ],
   "source": [
    "# 梯度衰减严重\n",
    "get_gradiant(model_without_shortcut,sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:layers.0.0.weight grad is 0.009734145365655422\n",
      "name:layers.1.0.weight grad is 0.03095017559826374\n",
      "name:layers.2.0.weight grad is 0.013045227155089378\n",
      "name:layers.3.0.weight grad is 0.007606383878737688\n",
      "name:layers.4.0.weight grad is 0.0726315826177597\n"
     ]
    }
   ],
   "source": [
    "# 梯度趋于稳定\n",
    "get_gradiant(model_with_shortcut,sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多头注意力\n",
    "# Listing 3.5 An efficient multi-head attention class\n",
    "# 多头注意力；关键理解view和transpose\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out%num_heads==0\n",
    "        self.num_heads=num_heads\n",
    "        self.d_in=d_in\n",
    "        self.d_out=d_out\n",
    "        self.head_dim=d_out//num_heads\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.w_q=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_k=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_v=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        \n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "        self.proj=nn.Linear(d_out,d_out)\n",
    "    def forward(self,x):\n",
    "        b,token_size,d_in=x.shape\n",
    "        query=self.w_q(x)\n",
    "        key=self.w_k(x)\n",
    "        # b,token_size,d_out\n",
    "        value=self.w_v(x)\n",
    "        \n",
    "        # 多头  \n",
    "        # b,token_size,heads,head_dim\n",
    "        query=query.view(b,token_size,self.num_heads,self.head_dim)\n",
    "        key=key.view(b,token_size,self.num_heads,self.head_dim)\n",
    "        value=value.view(b,token_size,self.num_heads,self.head_dim)\n",
    "        \n",
    "        # b,heads,token_size,head_dim\n",
    "        query=query.transpose(1,2)\n",
    "        key=key.transpose(1,2)\n",
    "        value=value.transpose(1,2)\n",
    "        \n",
    "        # b,heads,token_size,token_size\n",
    "        score=query@key.transpose(-1,-2)\n",
    "        score.masked_fill_(self.mask.bool()[:token_size,:token_size],-torch.inf)\n",
    "        \n",
    "        weight=torch.softmax(score/key.shape[-1]**0.5,dim=-1)\n",
    "        weight=self.dropout(weight)\n",
    "        \n",
    "        # b,heads,token_size,head_dim\n",
    "        context=weight@value\n",
    "        \n",
    "        return self.proj(context.contiguous().transpose(1,2).reshape(b,token_size,self.d_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "        d_in=cfg[\"emb_dim\"],\n",
    "        d_out=cfg[\"emb_dim\"],\n",
    "        context_length=cfg[\"context_length\"],\n",
    "        num_heads=cfg[\"n_heads\"],\n",
    "        dropout=cfg[\"drop_rate\"],\n",
    "        qkv_bias=cfg[\"qkv_bias\"])\n",
    "        \n",
    "        self.norm1=LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2=LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.feed=FeedForward(cfg)\n",
    "        self.drop=nn.Dropout(cfg['drop_rate'])\n",
    "    def forward(self,x):\n",
    "        shortcut=x\n",
    "        x=self.att(x)\n",
    "        x=self.norm1(x)\n",
    "        x=self.drop(x)\n",
    "        \n",
    "        x=x+shortcut\n",
    "        \n",
    "        shortcut=x\n",
    "        x=self.norm2(x)\n",
    "        x=self.feed(x)\n",
    "        x=self.drop(x)\n",
    "        \n",
    "        return x+shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)                    #A\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT_CONFIG_124M = {\n",
    "#     \"vocab_size\": 50257,    # Vocabulary size\n",
    "#     \"context_length\": 1024, # Context length\n",
    "#     \"emb_dim\": 768,         # Embedding dimension\n",
    "#     \"n_heads\": 12,          # Number of attention heads\n",
    "#     \"n_layers\": 12,         # Number of layers\n",
    "#     \"drop_rate\": 0.1,       # Dropout rate\n",
    "#     \"qkv_bias\": False       # Query-Key-Value bias\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.7 The GPT model architecture implementation\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.emb=nn.Embedding(num_embeddings=cfg['vocab_size'],embedding_dim=cfg['emb_dim'])\n",
    "        self.pos=nn.Embedding(num_embeddings=cfg['context_length'],embedding_dim=cfg['emb_dim'])\n",
    "        self.dropout=nn.Dropout(cfg['drop_rate'])\n",
    "        \n",
    "        \n",
    "        self.tf=nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "        )\n",
    "        self.norm=LayerNorm(cfg['emb_dim'])\n",
    "        self.fc=nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)\n",
    "    def forward(self,x):\n",
    "        b,token_size=x.shape\n",
    "        embed=self.emb(x)\n",
    "        posi=self.pos(torch.arange(token_size,device=x.device))\n",
    "        x=embed+posi\n",
    "        out=self.dropout(x)\n",
    "        out=self.tf(x)\n",
    "        out=self.norm(out)\n",
    "        out=self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6109, 3626, 6100,  345],\n",
       "        [6109, 3626, 6100,  345]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1474,  0.0156, -0.2362,  ...,  0.3034,  0.1746, -0.1839],\n",
       "         [ 0.2418, -1.0413, -0.3459,  ..., -0.0230,  0.5066, -0.6183],\n",
       "         [ 1.0513, -0.7881, -0.1305,  ..., -0.1484,  0.0746, -0.5877],\n",
       "         [-0.0426, -0.6350, -0.1387,  ...,  0.4113,  0.2560, -0.6925]],\n",
       "\n",
       "        [[ 0.5293, -0.2725, -0.3075,  ...,  0.2523,  0.6957, -0.8043],\n",
       "         [ 0.3081, -0.9317, -1.1856,  ...,  0.0471,  0.8333, -0.0140],\n",
       "         [ 0.7518, -0.8315, -0.2779,  ...,  0.3360,  0.5016, -0.1821],\n",
       "         [-0.2372, -0.5721, -0.1533,  ...,  0.8093,  0.6410, -0.6870]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model=GPTModel(GPT_CONFIG_124M)\n",
    "outs=model(batch)\n",
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "# 使用 numel() 方法（即 `number of elements` 的缩写），可以统计模型中参数张量的总参数量：\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257, 768])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原因在于 GPT-2 架构中使用了一种称为‘权重共享’的概念，这意味着 GPT-2 架构将 token 嵌入层的权重复用于输出层\n",
    "model.emb.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257, 768])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "end_params=total_params-sum([param.numel() for param in model.fc.parameters()])\n",
    "print(f\"Total number of parameters: {end_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'621.83203125MB'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算参数占用的内存 假设每位是float32，4个字节\n",
    "str(total_params*4/1024**2)+'MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.8 A function for the GPT model to generate text\n",
    "# 根据idx生成max_new_tokens长度的预测结果\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size): #A\n",
    "    b,token_size=idx.shape\n",
    "    for _ in range(max_new_tokens):\n",
    "        inputs=idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            # b,context_size,vocab_size\n",
    "            outputs=model(inputs)\n",
    "        # 只有最后一个预测的才有用\n",
    "        probs=outputs[:,-1,:]\n",
    "        probs=torch.softmax(probs,dim=-1)\n",
    "        outputs=probs.argmax(dim=-1,keepdim=True)\n",
    "        idx=torch.cat((idx,outputs),dim=1)\n",
    "        \n",
    "    return idx     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=tiktoken.get_encoding('gpt2')\n",
    "text=\"Hello, I am\"\n",
    "batch=torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval会省略dropout层\n",
    "model.eval()             #A\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=batch,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,    11,   314,   716,   349, 29448, 40144, 43496, 10262, 10262]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, I amol amen Kidd LuminAgAg'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out.squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mine",
   "language": "python",
   "name": "mine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
