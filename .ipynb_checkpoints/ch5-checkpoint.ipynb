{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\mine\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from chapter04 import GPTModel\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,        #A\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,             #B\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "#A 我们将上下文长度从1024个token缩短到256个token\n",
    "#B 将 dropout 设置为 0 是一种常见的做法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 5.1 Utility functions for text to token ID conversion\n",
    "import tiktoken\n",
    "from chapter04 import generate_text_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every effort moves you rentingetic wasnم refres RexMeCHicular stren'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# allowed_special={'<|endoftext|>'}\n",
    "start_context = \"Every effort moves you\"\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    ids=tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "    return torch.tensor(ids).unsqueeze(0)\n",
    "    \n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    text=tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "    return text\n",
    "tokenizer=tiktoken.get_encoding('gpt2')\n",
    "ids=text_to_token_ids(start_context,tokenizer)\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "token_ids_to_text(token_ids,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100], # [\"every effort moves\",\n",
    "                       [40, 1107, 588]])    # \"I really like\"]\n",
    "# Matching these inputs, the `targets` contain the token IDs we aim for the model to produce:\n",
    "targets = torch.tensor([[3626, 6100, 345 ], # [\" effort moves you\",\n",
    "                        [107, 588, 11311]]) # \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[16657],\n",
       "         [  339],\n",
       "         [42826]],\n",
       "\n",
       "        [[49906],\n",
       "         [29669],\n",
       "         [41751]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits=model(inputs)\n",
    "probas=torch.softmax(logits,dim=-1)\n",
    "outputs=probas.argmax(dim=-1,keepdim=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[16657,   339, 42826]])\n",
      " Armed heNetflix\n",
      "tensor([[49906, 29669, 41751]])\n",
      " pressuring empoweredfaith\n"
     ]
    }
   ],
   "source": [
    "for i in outputs:\n",
    "    print(i.reshape(1,-1))\n",
    "    print(token_ids_to_text(i.reshape(1,-1),tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([3.9108e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3626, 6100,  345])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx = 0\n",
    "\n",
    "# 三维向量索引规则，得到的是概率\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)\n",
    "targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -9.5042, -10.3796, -11.3677, -10.1492,  -9.7764, -12.2561])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cat在每行的列上拼接\n",
    "log_probas=torch.log(torch.cat((target_probas_1,target_probas_2)))\n",
    "log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.5722)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.5722)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.5722)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交叉熵损失函数\n",
    "torch.nn.functional.cross_entropy(logits.reshape(-1,logits.shape[-1]),targets.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20480\n",
      "Tokens: 5146\n"
     ]
    }
   ],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chapter02 import create_dataloader_v1\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch=input_batch.to(device)\n",
    "    target_batch=target_batch.to(device)\n",
    "    outputs=model(input_batch)\n",
    "    \n",
    "    # flatten可以合并维度\n",
    "    # 先放out再放target\n",
    "    loss=torch.nn.functional.cross_entropy(outputs.flatten(0,1),target_batch.flatten())\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 5.2 Function to compute the training and validation loss\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    loss_sum=0.0\n",
    "    if len(data_loader)==0:\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches=len(data_loader)\n",
    "    else:\n",
    "        num_batches=min(len(data_loader),num_batches)\n",
    "    for i,(x,y) in enumerate(data_loader):\n",
    "        if i>num_batches:\n",
    "            break\n",
    "        l=calc_loss_batch(x,y,model,device)\n",
    "        loss_sum+=l.item()\n",
    "    print(f'all loss is {loss_sum/num_batches}')\n",
    "    return loss_sum/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all loss is 10.987583372328016\n",
      "all loss is 10.987583266364204\n",
      "train_loss:10.987583372328016\n",
      "valid_loss:10.987583266364204\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss=calc_loss_loader(train_loader,model,device)\n",
    "    valid_loss=calc_loss_loader(train_loader,model,device)\n",
    "print(f'train_loss:{train_loss}')\n",
    "print('valid_loss:{0:}'.format(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# Listing 5.3 The main function for pretraining LLMs\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses,val_losses,track_tokens=[],[],[]\n",
    "    global_step=-1\n",
    "    token_seen=0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 每个epoch train\n",
    "        model.train()\n",
    "        for x,y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            l=calc_loss_batch(x,y, model=model, device=device)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            global_step+=1\n",
    "            token_seen+=x.numel()\n",
    "            \n",
    "            if global_step% eval_freq==0:\n",
    "                train_loss,val_loss=evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens.append(token_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "                \n",
    "        generate_and_print_sample(                                                 \n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return train_losses,val_losses,track_tokens\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss=calc_loss_loader(train_loader, model, device)\n",
    "        val_loss=calc_loss_loader(val_loader, model, device)\n",
    "    model.train()\n",
    "    return train_loss,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size=model.pos_emb.weight.shape[0]\n",
    "    encoded=text_to_token_ids(start_context,tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        text=token_ids_to_text(token_ids,tokenizer)\n",
    "    \n",
    "        print(text.replace('\\n',''))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all loss is 9.766464021470812\n",
      "all loss is 9.927428245544434\n",
      "Ep 1 (Step 000000): Train loss 9.766, Val loss 9.927\n",
      "all loss is 8.069049729241264\n",
      "all loss is 8.334250450134277\n",
      "Ep 1 (Step 000005): Train loss 8.069, Val loss 8.334\n",
      "Every effort moves you,,,,,,,,,,,,.\n",
      "all loss is 6.726918008592394\n",
      "all loss is 7.050800323486328\n",
      "Ep 2 (Step 000010): Train loss 6.727, Val loss 7.051\n",
      "all loss is 6.103990607791477\n",
      "all loss is 6.602636814117432\n",
      "Ep 2 (Step 000015): Train loss 6.104, Val loss 6.603\n",
      "Every effort moves you, the,, and,,,,,,, and,.\n",
      "all loss is 5.6177341143290205\n",
      "all loss is 6.502573013305664\n",
      "Ep 3 (Step 000020): Train loss 5.618, Val loss 6.503\n",
      "all loss is 5.43874979019165\n",
      "all loss is 6.3662309646606445\n",
      "Ep 3 (Step 000025): Train loss 5.439, Val loss 6.366\n",
      "Every effort moves you, and to the of the of the of the, and I had. Gis, and, and, and, and, and, and I had, and, and, and, and, and, and, and, and, and,\n",
      "all loss is 5.022319740719265\n",
      "all loss is 6.2828216552734375\n",
      "Ep 4 (Step 000030): Train loss 5.022, Val loss 6.283\n",
      "all loss is 4.599767790900336\n",
      "all loss is 6.31028413772583\n",
      "Ep 4 (Step 000035): Train loss 4.600, Val loss 6.310\n",
      "Every effort moves you of the picture.\"I\"IHe the picture\"I\"I\n",
      "all loss is 4.067726108762953\n",
      "all loss is 6.151906490325928\n",
      "Ep 5 (Step 000040): Train loss 4.068, Val loss 6.152\n",
      "Every effort moves you know the\"Oh, and his pictures a, and he said.\n",
      "all loss is 3.5733241770002575\n",
      "all loss is 6.196965217590332\n",
      "Ep 6 (Step 000045): Train loss 3.573, Val loss 6.197\n",
      "all loss is 3.0850657092200384\n",
      "all loss is 6.121931076049805\n",
      "Ep 6 (Step 000050): Train loss 3.085, Val loss 6.122\n",
      "Every effort moves you know the fact, and pushed one of the to the fact by his last word.\"Oh, and I was his pictures--I looked up at the fact, and up and down the room, I had\n",
      "all loss is 2.603932195239597\n",
      "all loss is 6.152874946594238\n",
      "Ep 7 (Step 000055): Train loss 2.604, Val loss 6.153\n",
      "all loss is 2.1968504322899713\n",
      "all loss is 6.1213603019714355\n",
      "Ep 7 (Step 000060): Train loss 2.197, Val loss 6.121\n",
      "Every effort moves you know,\" was one of the picture for a smile that I felt to see a so that he was a little to me to have to see a smile behind his pictures.\"--and it, the donkey. \"There were days when I\n",
      "all loss is 1.7638563050164118\n",
      "all loss is 6.139557361602783\n",
      "Ep 8 (Step 000065): Train loss 1.764, Val loss 6.140\n",
      "all loss is 1.3822382026248508\n",
      "all loss is 6.223080635070801\n",
      "Ep 8 (Step 000070): Train loss 1.382, Val loss 6.223\n",
      "Every effort moves you?\"\"Yes--quite insensible to the fact with a little: \"Yes--and by me to me to have to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I\n",
      "all loss is 1.0802646146880255\n",
      "all loss is 6.246952533721924\n",
      "Ep 9 (Step 000075): Train loss 1.080, Val loss 6.247\n",
      "all loss is 0.8190028137630887\n",
      "all loss is 6.280669212341309\n",
      "Ep 9 (Step 000080): Train loss 0.819, Val loss 6.281\n",
      "Every effort moves you?\"\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\"Oh, and I remember getting off a prodigious phrase about the honour being _mine_--because he didn't want\n",
      "all loss is 0.6112876534461975\n",
      "all loss is 6.394484519958496\n",
      "Ep 10 (Step 000085): Train loss 0.611, Val loss 6.394\n",
      "Every effort moves you?\"\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)      #A\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=1,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "#A .parameters() 方法返回模型的所有可训练权重参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax2 = ax1.twiny() #A\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0) #B\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "\n",
    "#A 创建与 y 轴共用的第二个 x 轴\n",
    "#B 用于对齐刻度的隐藏图形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forward'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob=torch.softmax(next_token_logits,dim=0)\n",
    "ids=prob.argmax(dim=-1)\n",
    "inverse_vocab[ids.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "# 采用temperature scaling：multinomial和temperature\n",
    "# temperature 值非常小时，生成的概率分布会更加尖锐，越大时，概率分布会更加均匀\n",
    "# 越大时，由于 exp 函数的敏感性较高，这意味着 logits 值的差异被“压平”，使得最优词的概率降低，而其他次优词的概率提高。输出的概率分布变得更加均匀\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "torch.multinomial(prob,num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.7500, 6.2800, 4.5100]) tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "# topk筛选最适合的前k个\n",
    "top_k=3\n",
    "# 返回值是value,index\n",
    "val,idx=torch.topk(next_token_logits,top_k)\n",
    "print(val,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用where将top_k之外的值置-inf\n",
    "torch.where(condition=next_token_logits<val[-1],input=torch.tensor(float('-inf')),other=next_token_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 5.4 A modified text generation function with more diversity\n",
    "def generate(model, idx, max_new_tokens, context_size,\n",
    "             temperature=1.0, top_k=None, eos_id=None):\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        id_con=idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            digits=model(id_con)\n",
    "        digits=digits[:,-1,:]\n",
    "        if top_k is not None:\n",
    "            top_digits,_=torch.topk(digits,top_k)\n",
    "            digits=torch.where(\n",
    "                condition=digits>top_digits[:,-1],\n",
    "                input=torch.tensor(float('-inf')).to(digits.device),\n",
    "                other=digits\n",
    "            )\n",
    "            \n",
    "        \n",
    "        if temperature>0.0:\n",
    "            digits=digits/temperature\n",
    "            digits=torch.softmax(digits,dim=-1)     \n",
    "            digits=torch.multinomial(digits,num_samples=1)\n",
    "        else:\n",
    "            digits=torch.softmax(digits,dim=-1)\n",
    "            digits=digits.argmax(dim=-1,keepdim=True)\n",
    "        idx=torch.cat((idx,digits),dim=1)\n",
    "        # 如果遇到序列结束token且指定了eos_id，则提前停止生成\n",
    "        if digits==eos_id:\n",
    "            break\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves youThen studio rewardedrow ConfederingAnnaenezYesSTAT Granted_> downed Mars Doyle\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数和优化器参数\n",
    "torch.save({\n",
    "    'model_state_dict':model.state_dict(),\n",
    "    'optimizer_state_dict':optimizer.state_dict()\n",
    "},'model_opt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载参数\n",
    "checkpoint=torch.load('model_opt.pth')\n",
    "model=GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=1e-3,weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████████████████████████████████████████████████████████████████████| 77.0/77.0 [00:00<?, ?iB/s]\n",
      "encoder.json: 100%|███████████████████████████████████████████████████████████████| 1.04M/1.04M [00:02<00:00, 416kiB/s]\n",
      "hparams.json: 100%|████████████████████████████████████████████████████████████████████████| 90.0/90.0 [00:00<?, ?iB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|███████████████████████████████████████████████| 498M/498M [12:12<00:00, 680kiB/s]\n",
      "model.ckpt.index: 100%|███████████████████████████████████████████████████████████| 5.21k/5.21k [00:00<00:00, 379kiB/s]\n",
      "model.ckpt.meta: 100%|██████████████████████████████████████████████████████████████| 471k/471k [00:01<00:00, 266kiB/s]\n",
      "vocab.bpe: 100%|████████████████████████████████████████████████████████████████████| 456k/456k [00:01<00:00, 286kiB/s]\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n",
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "#将 GPT-2 的架构设置（settings）和权重参数（params）加载到 Python 会话中\n",
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())\n",
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create a dictionary that lists the differences between the different GPT model sizes, as explained in Figure 5.17:\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "# Suppose we are interested in loading the smallest model, \"gpt2-small (124M)\". We can use the corresponding settings from the model_configs table able to update our full-length GPT_CONFIG_124M we defined and used earlier throughout the chapter as follows:\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "# 之前设置的 token 长度是 256，但 OpenAI 的原始 GPT-2 模型使用的是 1,024 的 token 长度，因此我们需要相应地更新 NEW_CONFIG:\n",
    "NEW_CONFIG.update({\"context_length\": 1024})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEW_CONFIG.update({\"qkv_bias\": True})\n",
    "# We can now use the updated NEW_CONFIG dictionary to initialize a new GPTModel instance:\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "def assign(left,right):\n",
    "    if left.shape!=right.shape:\n",
    "        raise ValueError(f'shape not consi{left.shape}:{right.shape}')\n",
    "    return nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 5.5 Loading OpenAI weights into our GPT model code\n",
    "import numpy as np\n",
    "#将 `params` 字典中的权重加载到 GPT 模型实例中：\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])               #A\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    for b in range(len(params[\"blocks\"])):                                       #B\n",
    "        q_w, k_w, v_w = np.split(                                                #C\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])                   #D\n",
    "\n",
    "\n",
    "#A 将模型的位置嵌入和token 嵌入的权重设置为 params 中指定的值\n",
    "#B 遍历模型中的每个 Transformer 模块\n",
    "#C 使用 np.split 函数将注意力和偏置权重分为三等份，分别用于查询、键和值组件\n",
    "#D OpenAI 的原始 GPT-2 模型在输出层中复用了 token 嵌入的权重，以减少参数总量，这一概念称为权重共享"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "device='cpu'\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you leaps shortly knock rapidly performTweet laterZBOX Space reductorsRe MayRe IV Opsluence yVR Analog BettyDr34Fire\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mine",
   "language": "python",
   "name": "mine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
