{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]]) # step     (x^6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs[1]                                               #A\n",
    "attn_scores = torch.empty(inputs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得score\n",
    "score=torch.matmul(inputs,inputs.T)\n",
    "# score 归一化\n",
    "weight=torch.softmax(score,dim=-1)\n",
    "weight\n",
    "# 得到上下文向量\n",
    "all_context_vecs=torch.matmul(weight,inputs)\n",
    "all_context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 3.1 A compact self-attention class\n",
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self,din,dout):\n",
    "        super().__init__()\n",
    "        self.w_q=nn.Parameter(torch.rand(din,dout))\n",
    "        self.w_k=nn.Parameter(torch.rand(din,dout))\n",
    "        self.w_v=nn.Parameter(torch.rand(din,dout))\n",
    "    def forward(self,x):\n",
    "        query=torch.matmul(x,self.w_q)\n",
    "        key=torch.matmul(x,self.w_k)\n",
    "        value=torch.matmul(x,self.w_v)\n",
    "        # 等价于矩阵相乘\n",
    "        score=query@key.T\n",
    "        # 将结果除以嵌入维度的平方根（即 key.shape[-1]），其中 key.shape[-1] 是嵌入向量的维度=2\n",
    "        # 这样可以将点积结果缩放到适当的范围，避免Softmax函数进入梯度平缓区，从而保持梯度的有效性，促进模型的正常训练。\n",
    "        weight=torch.softmax(score/key.shape[-1]**0.5,dim=-1)\n",
    "        context=weight@value\n",
    "        return context        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2996, 0.8053],\n",
       "        [0.3061, 0.8210],\n",
       "        [0.3058, 0.8203],\n",
       "        [0.2948, 0.7939],\n",
       "        [0.2927, 0.7891],\n",
       "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model=SelfAttention_v1(3,2)\n",
    "model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 3.1 A compact self-attention class\n",
    "#当禁用偏置单元时，`nn.Linear` 层可以有效地执行矩阵乘法。\n",
    "#此外，使用 `nn.Linear` 替代手动实现的 `nn.Parameter(torch.rand(...))` 的一个显著优势在于，\n",
    "# `nn.Linear` 具有优化的权重初始化方案，从而有助于实现更稳定和更高效的模型训练。\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self,din,dout,bias=False):\n",
    "        super().__init__()\n",
    "        self.w_q=nn.Linear(din,dout,bias=bias)\n",
    "        self.w_k=nn.Linear(din,dout,bias=bias)\n",
    "        self.w_v=nn.Linear(din,dout,bias=bias)\n",
    "    def forward(self,x):\n",
    "        query=self.w_q(x)\n",
    "        key=self.w_k(x)\n",
    "        value=self.w_v(x)\n",
    "        # 等价于矩阵相乘\n",
    "        score=query@key.T\n",
    "        weight=torch.softmax(score/key.shape[-1]**0.5,dim=-1)\n",
    "        context=weight@value\n",
    "        return context        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0739,  0.0713],\n",
       "        [-0.0748,  0.0703],\n",
       "        [-0.0749,  0.0702],\n",
       "        [-0.0760,  0.0685],\n",
       "        [-0.0763,  0.0679],\n",
       "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2=SelfAttention_v1(3,2)\n",
    "model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=sa_v2(inputs)\n",
    "key=sa_v2(inputs)\n",
    "value=sa_v2(inputs)\n",
    "score=query@key.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens=score.shape[0]\n",
    "# triu上对角线 diagonal=1表示对角线不填充1\n",
    "mask=torch.triu(torch.ones(lens,lens),diagonal=1)\n",
    "mask\n",
    "# 掩码必须是bool类型\n",
    "score=score.masked_fill(mask.bool(),-torch.inf)\n",
    "attn_weights=torch.softmax(score/key.shape[-1]**0.5,dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape\n",
    "# 模拟batch=2的输入\n",
    "batch=torch.stack((inputs,inputs),0)\n",
    "batch.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 3.3 A compact causal attention class\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.w_q=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_k=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_v=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        # register_buffer\n",
    "        # 当我们在大语言模型（LLM）中使用 `CausalAttention` 类时，buffer 会自动随模型迁移到合适的设备（CPU 或 GPU）。\n",
    "        # 这意味着我们无需手动确保这些张量与模型参数在同一设备上，从而避免设备不匹配错误。\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        b,token_size,dim=x.shape\n",
    "        query=self.w_q(x)\n",
    "        key=self.w_k(x)\n",
    "        value=self.w_v(x)\n",
    "        \n",
    "        score=query@key.transpose(-1,-2)\n",
    "        # masked_fill_的_表示原地操作\n",
    "        score.masked_fill_(self.mask.bool()[:token_size,:token_size],-torch.inf)\n",
    "        weight=torch.softmax(score/key.shape[-1]**0.5,dim=-1)\n",
    "        weight=self.dropout(weight)\n",
    "        context=weight@value\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 2])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "#  Dropout 通常应用于两个特定区域：计算注意力得分之后，或将注意力权重应用于 value 向量之后。\n",
    "# 在这里，我们会在计算完注意力权重之后应用 dropout 掩码\n",
    "# dropout会自动缩放：增大未遮盖值的相对差异；注意力分布会更集中（即更尖锐），让模型更关注特定的 token\n",
    "dropout=nn.Dropout(0.5)\n",
    "context_length=batch.shape[1]\n",
    "ca=CausalAttention(3,2,context_length,dropout)\n",
    "ca(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 3.4 A wrapper class to implement multi-head attention\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.head=nn.ModuleList(\n",
    "            [CausalAttention(d_in,d_out,context_length,dropout,qkv_bias) for _ in range(num_heads)]\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return torch.cat([head(x) for head in self.head],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 4])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout=nn.Dropout(0.5)\n",
    "mha=MultiHeadAttentionWrapper(3,2,batch.shape[1],dropout,2)\n",
    "end=mha(batch)\n",
    "end.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 3.5 An efficient multi-head attention class\n",
    "# 多头注意力；关键理解view和transpose\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out%num_heads==0\n",
    "        self.num_heads=num_heads\n",
    "        self.d_in=d_in\n",
    "        self.d_out=d_out\n",
    "        self.head_dim=d_out//num_heads\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.w_q=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_k=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_v=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        \n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "        self.proj=nn.Linear(d_out,d_out)\n",
    "    def forward(self,x):\n",
    "        b,token_size,d_in=x.shape\n",
    "        query=self.w_q(x)\n",
    "        key=self.w_k(x)\n",
    "        # b,token_size,d_out\n",
    "        value=self.w_v(x)\n",
    "        \n",
    "        # 多头  \n",
    "        # b,token_size,heads,head_dim\n",
    "        query=query.view(b,token_size,self.num_heads,self.head_dim)\n",
    "        key=key.view(b,token_size,self.num_heads,self.head_dim)\n",
    "        value=value.view(b,token_size,self.num_heads,self.head_dim)\n",
    "        \n",
    "        # b,heads,token_size,head_dim\n",
    "        query=query.transpose(1,2)\n",
    "        key=key.transpose(1,2)\n",
    "        value=value.transpose(1,2)\n",
    "        \n",
    "        # b,heads,token_size,token_size\n",
    "        score=query@key.transpose(-1,-2)\n",
    "        score.masked_fill_(self.mask.bool()[:token_size,:token_size],-torch.inf)\n",
    "        \n",
    "        weight=torch.softmax(score/key.shape[-1]**0.5,dim=-1)\n",
    "        weight=self.dropout(weight)\n",
    "        \n",
    "        # b,heads,token_size,head_dim\n",
    "        context=weight@value\n",
    "        \n",
    "        return self.proj(context.contiguous().transpose(1,2).view(b,token_size,self.d_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<AddBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],             #A\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "First head:\n",
      " torch.Size([3, 3])\n",
      "\n",
      "Second head:\n",
      " torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "print(first_head.shape)\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res.shape)\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mine",
   "language": "python",
   "name": "mine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
